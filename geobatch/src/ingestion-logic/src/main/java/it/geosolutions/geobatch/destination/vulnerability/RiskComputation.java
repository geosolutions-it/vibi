/*
 *  Copyright (C) 2007-2012 GeoSolutions S.A.S.
 *  http://www.geo-solutions.it
 *
 *  GPLv3 + Classpath exception
 *
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation, either version 3 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */
package it.geosolutions.geobatch.destination.vulnerability;

import it.geosolutions.destination.utils.Formula;
import it.geosolutions.destination.utils.FormulaUtils;
import it.geosolutions.geobatch.destination.common.InputObject;
import it.geosolutions.geobatch.destination.common.OutputObject;
import it.geosolutions.geobatch.destination.ingestion.MetadataIngestionHandler;
import it.geosolutions.geobatch.flow.event.ProgressListenerForwarder;

import java.io.IOException;
import java.sql.Connection;
import java.util.HashMap;
import java.util.Map;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.geotools.data.DataStore;
import org.geotools.data.Transaction;
import org.geotools.feature.NameImpl;
import org.geotools.feature.simple.SimpleFeatureBuilder;
import org.geotools.feature.simple.SimpleFeatureTypeBuilder;
import org.opengis.feature.simple.SimpleFeature;
import org.opengis.feature.simple.SimpleFeatureType;
import org.opengis.filter.Filter;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This class is the entry point for the vulnerability process (method
 * computeVulnerability) and it implements the computation's main loop
 * 
 * @author Alessio Fabiani - <alessio.fabiani at geo-solutions.it>
 * 
 */
public class RiskComputation extends InputObject {

	private final static Logger LOGGER = LoggerFactory
			.getLogger(RiskComputation.class);

	private static Pattern TYPE_NAME_PARTS = Pattern
			.compile("^([A-Z]{2})_([A-Z]{1})_([A-Za-z]+)_([0-9]{8})(_.*?)?$");

	public static String ARC_INPUT_TYPE_NAME_LN = "siig_geo_ln_arco_X";
	public static String ARC_INPUT_TYPE_NAME_PL = "siig_geo_pl_arco_X";

	public static String RISK_OUTPUT_TYPE_NAME = "siig_t_elab_standard_X";
	public static String GEOID = "id_geo_arco";
	
	public static String PARTNER_FIELD = "fk_partner";
	
	String codicePartner;
    int partner;
    private String date;

	/**
	 * @param inputTypeName
	 * @param listenerForwarder
	 */
	public RiskComputation(String inputFeature, ProgressListenerForwarder listenerForwarder,
			MetadataIngestionHandler metadataHandler, DataStore dataStore) {
		super(inputFeature, listenerForwarder,
				metadataHandler, dataStore);
		// default area
	}
	
	@Override
	protected String getInputTypeName(String inputTypeName) {
		return inputTypeName.replace("_ORIG", "").replace("_CALCS", "");
	}

	@Override
	protected boolean parseTypeName(String typeName) {
		Matcher m = TYPE_NAME_PARTS.matcher(typeName);
        if(m.matches()) {
			// partner alphanumerical abbreviation (from siig_t_partner)
			codicePartner = m.group(1);
			// partner numerical id (from siig_t_partner)
			partner = Integer.parseInt(partners.get(codicePartner).toString());			
			date = m.group(4);
			
			return true;
		}
        return false;
	}

	private static String getTypeName(String typeName, int aggregationLevel) {
		return typeName.replace("X", aggregationLevel + "");
	}

    private Long startOriginId; 
    private Long endOriginId;
    private Long totPages; 
    private Long pageNumber;
    
    /**
     * @param startOriginId the startOriginId to set
     */
    public void setStartOriginId(Long startOriginId) {
        this.startOriginId = startOriginId;
    }

    /**
     * @param endOriginId the endOriginId to set
     */
    public void setEndOriginId(Long endOriginId) {
        this.endOriginId = endOriginId;
    }

    /**
     * @param totPages the totPages to set
     */
    public void setTotPages(Long totPages) {
        this.totPages = totPages;
    }

    /**
     * @param pageNumber the pageNumber to set
     */
    public void setPageNumber(Long pageNumber) {
        this.pageNumber = pageNumber;
    }
    
	/**
	 * Pre-computes the risks and stores the outcomes on the DB.
	 * 
	 * @param datastoreParams
	 * @param batch
	 *            batch calculus size
	 * @param precision
	 *            output value precision (decimals)
	 * @param aggregationLevel
	 *            input arcs level
	 * @param processing
	 *  		  id of the processing type
	 * @param formula
	 *            id of the formula to calculate
	 * @param target
	 *            id of the target/s to use in calculation
	 * @param materials
	 *            ids of the materials to use in calculation
	 * @param scenarios
	 *            ids of the scenarios to use in calculation
	 * @param entities
	 *            ids of the entities to use in calculation
	 * @param severeness
	 *            ids of the severeness to use in calculation
	 * @param fpfield
	 * 			  fields to use for fp calculation
	 * @throws IOException
	 */
	public void prefetchRiskAtLevel(Integer precision, int aggregationLevel, int processing, int formula, int target, String materials, String scenarios,
			String entities, String severeness, String fpfield, String writeMode, String closePhase, boolean dropInput, boolean newProcess) throws IOException {
            reset();
            if (isValid()) {
                if (precision == null) {
                    precision = 4;
                }
    
                // read input features
                String outputFeatureName = getTypeName(RISK_OUTPUT_TYPE_NAME, aggregationLevel);
    
                int process = -1;
        		int trace = -1;
        		
        		int errors = 0;
        		int startErrors = 0;
    
        		try {
        			
        			// create or retrieve metadata for ingestion
        			if(newProcess) {
        				removeOldImports();
        				// new process
        				process = createProcess();
        				// write log for the imported file
        				trace = logFile(process, NO_TARGET,
        						partner, codicePartner, date, false);
        			} else {
        				// existing process
        				MetadataIngestionHandler.Process importData = getProcessData();
        				if (importData != null) {
        					process = importData.getId();
        					trace = importData.getMaxTrace();
        					errors = importData.getMaxError();
        					startErrors = errors;
        				}
        			}
                
	                if (process == -1) {
	                    LOGGER.error("Cannot find process for input file");
	                    throw new IOException("Cannot find process for input file");
	                }
	                
	                String inputTypeName = getTypeName(aggregationLevel >= 3 ? ARC_INPUT_TYPE_NAME_PL
	                        : ARC_INPUT_TYPE_NAME_LN, aggregationLevel);
	                createInputReader(dataStore, Transaction.AUTO_COMMIT, inputTypeName);
	    
	                Filter partnerFilter = filterFactory.equals(filterFactory.property(PARTNER_FIELD),
	                        filterFactory.literal(partner));
	                setInputFilter(partnerFilter);
	    
	                // calculates total objects to import
	                int total = getImportCount();
	                
	                VulnerabilityOperation concreteOperation = VulnerabilityOperation
	                        .instantiateWriterFromString(writeMode);
	                LOGGER.info("Using writer " + VulnerabilityOperation.class);
	                
	                // Setup filtering
	                concreteOperation.setStartOriginId(startOriginId);
	                concreteOperation.setEndOriginId(endOriginId);
	                concreteOperation.setPageNumber(pageNumber);
	                concreteOperation.setTotPages(totPages);
	                OutputObject riskObj = new OutputObject(dataStore, Transaction.AUTO_COMMIT, outputFeatureName,
	                        GEOID);
	                
	                try {
	                	// remove old features
		                concreteOperation.initFeature(riskObj, partner);
	                } catch (IOException e) {
						errors++;	
						metadataHandler.logError(trace, errors, "Error removing old data", getError(e), 0);					
						throw e;
	                }
        			
	                // we will calculate risk in batch of arcs
                    // we store each feature of the batch in a map
                    // indexed by id
                    // ids will store the list of id of each batch
                    // used to build risk query
                    
                    setInputFilter(concreteOperation.buildOriginFilter(partner, total));
                    LOGGER.info("Start computation: ThreadName: " + Thread.currentThread().getName()
                            + " - startOriginId: " + startOriginId + " - endOriginId: " + endOriginId);
    
                    
                    Connection conn = getConnection(dataStore,Transaction.AUTO_COMMIT);
                    
                    try {
                    	if (conn != null) {                                                        
                            Formula formulaDescriptor = Formula.load(conn, processing, formula, target);
                            /*if ((!formulaDescriptor.hasGrid() && aggregationLevel == 3)
                                    || (!formulaDescriptor.hasNoGrid() && aggregationLevel < 3)) {
                                LOGGER.info("Formula not supported on this level, returning empty collection");
                            } else {*/
                            	
                            SimpleFeatureBuilder fb = createFeatureTypeBuilder(riskObj);
                        	
                        	// iterate source features
                            
                            SimpleFeature inputFeature = null;
                            while ((inputFeature = readInput()) != null) {
                            	
                            	Double[] risk = new Double[] { 0.0, 0.0 };
                                Number id = (Number) inputFeature.getAttribute("id_geo_arco");
                                final long flg_lieve = 1;
                                final long id_geo_arco = id.longValue();
                                final long id_distanza = 1;
                                final long id_scenario = 1;
                                final long id_sostanza = 9;

                                final String fid = flg_lieve + "." + id_geo_arco + "."
                                        + id_distanza + "." + id_scenario + "." + id_sostanza;
                                
                                Map<Number, SimpleFeature> temp = new HashMap<Number, SimpleFeature>();
                                // calculate risk here only if it depends from arcs
                                temp.put(id.intValue(), fb.buildFeature(fid));
                                
                                Map<String, Double> statsMap = new HashMap<String, Double>();
                                try {
                                	FormulaUtils.calculateFormulaValues(conn, aggregationLevel,
                                            processing, formulaDescriptor, id_geo_arco + "", partner
                                                    + "", materials, materials, scenarios, entities, severeness,
                                            fpfield, target, temp, precision, false);
                                	
                                	
                                	if (temp != null && !temp.isEmpty()) {
                                        statsMap.put("flg_lieve", new Double(flg_lieve));
                                        statsMap.put("id_geo_arco", new Double(id_geo_arco));
                                        statsMap.put("id_distanza", new Double(id_distanza));
                                        statsMap.put("id_scenario", new Double(id_scenario));
                                        statsMap.put("id_sostanza", new Double(id_sostanza));
    
                                        statsMap.put("fk_partner", new Double(partner));
    
                                        risk[0] = (Double) (temp.get(id.intValue()).getAttribute(
                                                "rischio1") != null ? temp.get(id.intValue())
                                                .getAttribute("rischio1") : risk[0]);
                                        risk[1] = (Double) (temp.get(id.intValue()).getAttribute(
                                                "rischio2") != null ? temp.get(id.intValue())
                                                .getAttribute("rischio2") : risk[1]);
    
                                        statsMap.put("calc_formula_soc", risk[0]);
                                        statsMap.put("calc_formula_amb", risk[1]);
                                    }
                                } catch (Exception e) {
                                    LOGGER.error("Error calculating risk on "
                                            + outputFeatureName + " for arc " + id, e);
                                    errors++;
                                    metadataHandler.logError(trace, errors,
                                    		"Error calculating risk on "
                                                    + outputFeatureName + " for arc " + id,
                                            getError(e), id.intValue());

                                }
                                
                                if (!statsMap.isEmpty()) {
                                    try {
                                        LOGGER.debug("Computed Risk for output feature ["
                                                + outputFeatureName + "]:" + fid
                                                + " - Risk values [" + risk[0] + ";" + risk[1]
                                                + "]");
                                            concreteOperation.writeOutputObjects(trace, riskObj, total,
                                                    outputFeatureName, inputFeature, fid, statsMap,
                                                    partner);
   
                                        } catch (Exception e) {
                                            LOGGER.error("Error writing objects on "
                                                + outputFeatureName, e);
                                        errors++;
                                        metadataHandler.logError(trace, errors,
                                                "Error writing objects on " + outputFeatureName,
                                                getError(e), id.intValue());

                                    }
                                }
                                updateImportProgress(total, errors - startErrors, "Precalculating risk in " + outputFeatureName);
                            }
                        }
                    	//}
	        			importFinished(total, errors - startErrors, "Data imported in " + outputFeatureName);
	        			
                    } finally {
                    	closeInputReader();
                    	if(conn != null) {
                    		conn.close();
                    	}
                    }
                } catch (Exception e) {
                	LOGGER.error(e.getMessage(), e);
                    errors++;
                    metadataHandler.logError(trace, errors, "Error calculating risk on "
                            + outputFeatureName, getError(e), 0);
                   
                } finally {
                	
                    if(dropInput) {
    					dropInputFeature(dataStore);
    				}
                    finalReport(errors - startErrors);
                    if (process != -1 && closePhase != null) {
                        // close current process phase
                        metadataHandler.closeProcessPhase(process, closePhase);
                    }
                    //transaction.close();
                }
    
            }
	}

	private SimpleFeatureBuilder createFeatureTypeBuilder(OutputObject riskObj) {
		SimpleFeatureTypeBuilder tb = new SimpleFeatureTypeBuilder();
		tb.add("id_geo_arco", riskObj.getSchema().getDescriptor("id_geo_arco").getType()
		        .getBinding());
		// tb.add("geometria", MultiLineString.class,riskObj.getSchema().getGeometryDescriptor().getCoordinateReferenceSystem());
		tb.add("rischio1", Double.class);
		tb.add("rischio2", Double.class);
		// fake layer name (risk) used for WPS output. Layer risk must be defined in GeoServer
		// catalog
		tb.setName(new NameImpl(riskObj.getSchema().getName().getNamespaceURI(), "risk"));
		SimpleFeatureType ft = tb.buildFeatureType();
		// result builder
		SimpleFeatureBuilder fb = new SimpleFeatureBuilder(ft);
		return fb;
	}
	
	
}
